{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108 entries, 0 to 107\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Date     108 non-null    object\n",
      " 1   Website  108 non-null    object\n",
      " 2   URL      108 non-null    object\n",
      " 3   Snippet  103 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.5+ KB\n"
     ]
    }
   ],
   "source": [
    "a=pd.read_csv(\"../Data/Press Coverage.csv\")\n",
    "a.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from OpenAIKey import openAIKey\n",
    "\n",
    "key=openAIKey.Key\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from OpenAI:\n",
      "In realms of loops and code so grand,\n",
      "There lies a concept, truly unplanned.\n",
      "Recursion, they call it, a clever tool,\n",
      "To break down problems, no longer cruel.\n",
      "\n",
      "A function that calls itself with glee,\n",
      "Unraveling complexity, setting minds free.\n",
      "Like a mirror reflecting its own gaze,\n",
      "Recursion delves deep in a mesmerizing maze.\n",
      "\n",
      "A base case, a condition to break the chain,\n",
      "Ensures the loop does not in vain.\n",
      "Each step returning, smaller in size,\n",
      "Until the problem no longer lies.\n",
      "\n",
      "So code unfolds like branches of a tree,\n",
      "With recursion's magic, elegant and free.\n",
      "Infinite depth, yet finite in scope,\n",
      "Recursion in programming, a method to elope. \n",
      "\n",
      "Redefining problems, with elegance rare,\n",
      "Recursion in programming, a virtuoso affair.\n",
      "In patterns of repetition, a dance so sweet,\n",
      "Recursion whispers secrets, making challenges meet.\n"
     ]
    }
   ],
   "source": [
    "from OpenAICall import OpenAICall\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"Compose a poem that explains the concept of recursion in programming.\"\n",
    "\n",
    "# 调用 OpenAICall 函数\n",
    "response = OpenAICall(system_prompt, user_prompt)\n",
    "\n",
    "# 打印返回的消息内容\n",
    "print(\"Response from OpenAI:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith can help with testing in several ways:\\n\\n1. Automated Testing: Langsmith can generate test scripts for automated testing frameworks like Selenium, JUnit, or TestNG. This can help streamline the testing process and ensure better test coverage.\\n\\n2. Test Data Generation: Langsmith can generate realistic test data for different scenarios, making it easier to test edge cases and ensure robustness of the software.\\n\\n3. Test Case Generation: Langsmith can generate test cases based on requirements or specifications, helping to ensure that all functionalities are tested thoroughly.\\n\\n4. Regression Testing: Langsmith can automate the execution of regression tests, allowing for quick validation of new changes without the need for manual intervention.\\n\\n5. Performance Testing: Langsmith can generate load tests to simulate real-world usage scenarios and identify performance bottlenecks in the software.\\n\\nOverall, Langsmith can significantly improve the efficiency and effectiveness of the testing process by automating repetitive tasks and providing valuable insights into the quality of the software.', response_metadata={'token_usage': {'completion_tokens': 195, 'prompt_tokens': 28, 'total_tokens': 223}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5649e7ef-d829-4420-a4e3-f5e1ce6a6b55-0', usage_metadata={'input_tokens': 28, 'output_tokens': 195, 'total_tokens': 223})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can help with testing in several ways:\\n\\n1. Test Automation: Langsmith can be used to automate testing processes, allowing for faster and more efficient testing of software products. By writing test scripts and scenarios in Langsmith, testers can easily run automated tests across different platforms and environments.\\n\\n2. Performance Testing: Langsmith can also be used to conduct performance testing of software applications. Testers can simulate high user loads and stress conditions to analyze the performance of the application and identify any bottlenecks or performance issues.\\n\\n3. Data Generation: Langsmith can help in generating test data for different scenarios, making it easier for testers to create realistic test cases. This can help in identifying edge cases and potential issues that might not be apparent with limited test data.\\n\\n4. Integration Testing: Langsmith can be used to facilitate integration testing by enabling testers to create and execute tests that verify the interaction between different components of the software system. This can help in ensuring that all components work together seamlessly.\\n\\nOverall, Langsmith can be a valuable tool in the testing process, helping testers to improve the quality of software products and ensure their reliability and performance.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Frank name meaning and origin\"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Your name is Frank.\"), AIMessage(content=\"Ok! My name is Frank now\")]\n",
    "chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"What's your name\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
